{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter User Archetypes with NLP\n",
    "_The task: build a Twitter bot that can determine a user’s interest on the fly and serve up a relevant GIF._\n",
    "\n",
    "## Preparing the Timelines\n",
    "To start we need a method of collecting and processing timelines, we’ll be using the wonderful [Tweepy](http://www.tweepy.org/) package to query the Twitter API. Below is a simple function for collecting a given Twitter account’s timeline as a list of strings — we do not need the full tweet object for each tweet as we’re only working on with the text.\n",
    "\n",
    "_NOTE: The Twitter API returns “up to” 200 tweets, because of this it may return fewer than 200 tweets, we want the option to either make only one call (`goal = -1`) or however many calls are required to reach a given number of tweets (`goal = 1000`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from time import sleep\n",
    "\n",
    "twitter_auth_APP = tweepy.AppAuthHandler(\n",
    "    '<CONSUMER_KEY>',\n",
    "    '<CONSUMER_SECRET>'\n",
    ")\n",
    "\n",
    "API = tweepy.API(twitter_auth_APP)\n",
    "\n",
    "def getTimeline(handle, exclude_rts = False, exclude_replies = False, goal = -1):\n",
    "    print(\"Collecting {}'s timeline\".format(handle))\n",
    "    timeline = []\n",
    "    max_id = None\n",
    "    \n",
    "    while True:\n",
    "        new_tweets = API.user_timeline(\n",
    "            handle,\n",
    "            count = 200,\n",
    "            tweet_mode = 'extended',\n",
    "            exclude_replies = exclude_replies,\n",
    "            include_rts = not exclude_rts,\n",
    "            max_id = max_id\n",
    "        )\n",
    "        \n",
    "        # check that the call returned tweets\n",
    "        if (max_id and len(new_tweets) == 1) or len(new_tweets) == 0:\n",
    "            break\n",
    "            \n",
    "        # add the new tweets to the timeline\n",
    "        timeline += new_tweets[1:]\n",
    "        # set the max_id for the next call\n",
    "        max_id = new_tweets[-1].id_str\n",
    "        \n",
    "        if goal < 1:\n",
    "            break\n",
    "            \n",
    "        elif len(timeline) >= goal:\n",
    "            break\n",
    "\n",
    "    # extract the string from the timeline object\n",
    "    timeline_as_strings = [tweet.full_text for \\\n",
    "                           tweet in timeline[:goal]]\n",
    "    \n",
    "    return timeline_as_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to clean and tokenize each tweet. We’ll remove hashtags (these are usually brand or event specific), at-mentions, numbers, and common words (basic [stop words](https://en.wikipedia.org/wiki/Stop_words) provided by [NLTK](http://www.nltk.org/) and others identified by manually analyzing timelines) that could skew our results. We’ll also separate each tweet into a list of 1-, 2-, and 3-word strings (known as [n-grams](https://en.wikipedia.org/wiki/N-gram)).\n",
    "\n",
    "_NOTE: Manually analyzing timelines is a pain, the best method I’ve found is to tokenize each timeline then look at the top 10 most common tokens and their frequency for each timeline and add terms to your stop list that are disproportionately frequent._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits\n",
    "\n",
    "stoplist_tw=['get', 'got', 'hey', 'hmm', 'hoo', 'let', 'ooo', 'par',\n",
    "    'yer', 'didn', 'one', 'com', 'new', 'like', 'great',\n",
    "    'make', 'top', 'awesome', 'best', 'good', 'wow', 'yes',\n",
    "    'say', 'yay', 'would', 'thanks', 'thank', 'use',\n",
    "    'should', 'could','best','really','see','want','nice',\n",
    "    'while','know', 'trump', 'nyfw', 'iphone', 'hurricane',\n",
    "    'rt', 'per', 'espn', 'soundcloud', 'ten', 'count', \n",
    "    'advance', 'newsletter','thedish', 'nycwff', 'thefword',\n",
    "    'irma', 'fave', 'beer', 'stefan', 'aiga',\n",
    "    'aigatogether', 'aigadesignconf', 'aigadg', 'aigagala',\n",
    "    'ddc', 'tbt', 'whitneybiennial', 'calder', 'wknd',\n",
    "    'pipilottirist', 'live', 'watch', 'check', 'video',\n",
    "    'clip', 'today', 'tonight', 'week', 'year', 'month',\n",
    "    'time', 'last', 'night', 'morning', 'yesterday',\n",
    "    'tomorrow', 'day', 'first', 'love', 'nyc', 'city',\n",
    "    'york', 'new', 'happy', 'need', 'look', 'back', 'right',\n",
    "    'win', 'chance', 'enter', 'ever','pst','wha','yep', 'via',\n",
    "    'app', 'twitter', 'streaming', 'stream', 'ask', 'amp',\n",
    "    'beautiful', 'best', 'amazing', 'good', 'perfect', 'cute',\n",
    "    'simple', 'love'\n",
    "]\n",
    "stoplist = set(stopwords.words('english') + stoplist_tw)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(document, max_n_grams = 3, min_word_length = 3):\n",
    "    # removes hashtags, at-mentions, and links\n",
    "    document = re.sub(\n",
    "        r\"(?:#|\\@|https?\\:\\/\\/|www\\.)\\S+\",\n",
    "        \"\",\n",
    "        document\n",
    "    )\n",
    "    \n",
    "    tokens = tokenizer.tokenize(document.lower())\n",
    "    \n",
    "    # remove tokens that are shorter than the min_word_length\n",
    "    # remove tokens that contain numbers\n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens\n",
    "        if (len(token) >= min_word_length) and\n",
    "        (len(token.strip(digits)) == len(token)) and\n",
    "        (token not in stoplist)\n",
    "    ]\n",
    "    \n",
    "    # collect n_grams up to the max_n_gram size\n",
    "    token_grams = []\n",
    "    if (max_n_grams > 1):\n",
    "        for n in range(2, max_n_grams + 1):\n",
    "            for start, token in enumerate(tokens[:-1*(n-1)]):\n",
    "                gram = ''\n",
    "                end = start + n\n",
    "                for token in tokens[start:end]:\n",
    "                    gram += ' ' + token\n",
    "                    \n",
    "                token_grams.append(gram.strip())\n",
    "                \n",
    "    return tokens + token_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a JSON file to load in the prototypes for each archetype with a field declaring whether to include or exclude retweets (I found these were sometimes noisy). This file had the following format:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"handle\": \"<HANDLE>\",\n",
    "        \"category\": \"<CATEGORY>\",\n",
    "        \"exclude_rts\": <true OR false>\n",
    "    },\n",
    "    ...\n",
    "    {\n",
    "        \"handle\": \"<HANDLE>\",\n",
    "        \"category\": \"<CATEGORY>\",\n",
    "        \"exclude_rts\": <true OR false>\n",
    "    }\n",
    "]\n",
    "```\n",
    "Finally, we gather 2000 tweets from each prototypical account, excluding replies as these are brands and replies are largely used for customer service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "ground_truth_file = 'ground_truth.json'\n",
    "ground_truth = None\n",
    "with open(ground_truth_file, 'r') as data:\n",
    "     ground_truth = json.loads(data.read())\n",
    "\n",
    "timelines = []\n",
    "categories = []\n",
    "handles = []\n",
    "for user in ground_truth:\n",
    "    # store categories and handles for later\n",
    "    categories.append(user['category'])\n",
    "    handles.append(user['handle'])\n",
    "    \n",
    "    timeline = getTimeline(\n",
    "        user['handle'],\n",
    "        user['exclude_rts'],\n",
    "        True, # exclude noisy replies\n",
    "        2000\n",
    "    )\n",
    "    \n",
    "    tokenized_timeline = [tokenize(tweet) for tweet in timeline]\n",
    "    timelines.append(tokenized_timeline)\n",
    "\n",
    "del ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Topic Model\n",
    "Now that we have each timeline we can start training the topic model. When I was first working on this project, I tried training the model with each timeline as a document. I found that, given the lack of actual paragraphs on Twitter this was far too noisy. Instead, I trained the model with each individual tweet as a document. This tightened the topics substantially.\n",
    "\n",
    "We’ll be using the Gensim package for the topic modeling. We start by converting the tweets to a bag-of-words corpus to simplify each tweet to a vector representation of word IDs and their respective frequency: `[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]`. In this step we can also trim out words that are both too frequent (present in greater than 5% of all tweets) and too infrequent (present in fewer than 5 tweets). To ensure that we are properly weighting words despite all our cleaning, we apply a [term frequency-inverse document frequency (tf-idf)](https://radimrehurek.com/gensim/models/tfidfmodel.html) transformation to our corpus. Given the size of our documents (140 characters or fewer at the time), the tf-idf transformation won’t alter our training data much, but will be wildly helpful when running the model on the full timelines. Finally, we train our topic model. In this case I created a [latent semantic indexing (LSI)](https://radimrehurek.com/gensim/models/lsimodel.html) model — more often referred to as latent semantic analysis (LSA). LSI/LSA is a method of dimensionality reduction that uses [singular-value decomposition](https://en.wikipedia.org/wiki/Singular-value_decomposition) to group words into topics, the basic assumption behind this is that similar words appear in similar documents. Unfortunately the method of determining the number of topics is often a process of trial and error involving analyzing the words in sample topics to verify that they make sense, I ended up using 40 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "all_tweets = []\n",
    "for timeline in timelines:\n",
    "    all_tweets.extend(timeline)\n",
    "\n",
    "dictionary = corpora.Dictionary(all_tweets)\n",
    "# filter out any terms only present in fewer than 5 tweets\n",
    "# or present in greater than 5% of all tweets\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.05)\n",
    "dictionary.compactify()\n",
    "# convert tweets to bag of words\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in all_tweets]\n",
    "\n",
    "# train the tfidf model\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# train the LSI/LSA model\n",
    "lsi = LsiModel(\n",
    "    corpus = tfidf[corpus],\n",
    "    id2word = dictionary,\n",
    "    num_topics = 40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily build an LSI vector representation of each timeline using the topic model. While we trained the model on each individual tweet we will be vectorizing each timeline as a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_vectors = []\n",
    "for i, timeline in enumerate(timelines):\n",
    "    # string together all tweets in each timeline\n",
    "    timeline_flat = [token for tweet in timeline \\\n",
    "                     for token in tweet]\n",
    "    # convert to bag of words\n",
    "    timeline_bow = dictionary.doc2bow(timeline_flat)\n",
    "    timeline_vector = lsi[tfidf[timeline_bow]]\n",
    "    timeline_vectors.append(timeline_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize these vectors using [principal component analysis (PCA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) via [scikit-learn](http://scikit-learn.org/stable/) and [matplotlib](https://matplotlib.org/). Gensim outputs its vectors (list of tuples) differently than scikit-learn (NumPy array) so we need to convert them for cross-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# converts a list of tuples to a dense NumPy array\n",
    "def tuples_array(tuples_vector, length):\n",
    "    arr = [0.0 for _ in range(length)]\n",
    "    for col, val in tuples_vector:\n",
    "        arr[col] = val\n",
    "    return np.array(arr)\n",
    "\n",
    "# convert text labels to ids\n",
    "encoder = LabelEncoder()\n",
    "categories_encoded = encoder.fit_transform(categories)\n",
    "\n",
    "timeline_arrays = np.array([tuples_array(vector, lsi.num_topics) for \\\n",
    "                   vector in timeline_vectors])\n",
    "labels = encoder.classes_\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_pca = pca.fit(timeline_arrays).transform(timeline_arrays)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "cmap = plt.get_cmap('tab20b_r')\n",
    "colors = cmap(np.linspace(0,1,len(encoder.classes_)))\n",
    "\n",
    "for color, i, label in zip(colors, range(len(encoder.classes_)), \\\n",
    "                           encoder.classes_):\n",
    "    plt.scatter(\n",
    "        X_pca[categories_encoded == i, 0],\n",
    "        X_pca[categories_encoded == i, 1],\n",
    "        color = color,\n",
    "        alpha = 1,\n",
    "        lw = 4,\n",
    "        label = label\n",
    "    )\n",
    "    \n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.ylim(-0.75, 0.75)\n",
    "plt.xlim(-0.75, 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Categorization Model\n",
    "We have topic vectors for every account in each category and we can see that even at two dimensions these categories are distinct. We can now use these vectors to build a model to categorize new users based on their topic distribution. For our categorization algorithm we’ll use [k-nearest neighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), to compare each new timeline to our prototypes. We’ll use [cosine similarity](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html), because it looks at the angle between any two vectors, this way it compares orientation of the vectors agnostic of magnitude. We’ll then use [cross-validated grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to test out some options for the number of neighbors (k) and the weighting of each of those neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': range(5, 20),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': [cosine]\n",
    "}\n",
    "neigh = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(neigh, param_grid, cv = 10)\n",
    "grid_search.fit(timeline_arrays, categories_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "testing_file = 'testing.json'\n",
    "tsting_data = None\n",
    "with open(testing_file, 'r') as data:\n",
    "     tsting_data = json.loads(data.read())\n",
    "\n",
    "test_timelines = []\n",
    "test_categories = []\n",
    "test_handles = []\n",
    "for user in tsting_data:\n",
    "    test_categories.append(user['category'])\n",
    "    test_handles.append(user['handle'])\n",
    "    \n",
    "    timeline = getTimeline(user['handle']\n",
    "    \n",
    "    tokenized_timeline = [tokenize(tweet) for tweet in timeline]\n",
    "    test_timelines.append(tokenized_timeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll format this for the model and get an accuracy sore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = []\n",
    "for timeline in test_timelines:\n",
    "    all_tokens = [token for tweet in timeline for token in tweet]\n",
    "    test_vectors.append(lsi[tfidf[dictionary.doc2bow(all_tokens)]])\n",
    "test_arrays = np.array([tuples_array(vector, lsi.num_topics) for vector in test_vectors])\n",
    "test_categories_encoded = encoder.transform(test_categories)\n",
    "\n",
    "grid_search.score(test_arrays, test_categories_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up with an accuracy of ~75%! Finally let’s save our model to disk and build the bot. We’ll use the ever-handy [joblib](https://pythonhosted.org/joblib/) pickler as well as Gensim’s built in serialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# save model\n",
    "# create the model directory if it doesn't exist\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "dictionary.save('model/dictionary.pkl')\n",
    "tfidf.save('model/tfidf.pkl')\n",
    "lsi.save('model/lsi.pkl')\n",
    "joblib.dump(encoder, 'model/encoder.pkl')\n",
    "joblib.dump(grid_search, 'model/grid_search.pkl')\n",
    "\n",
    "# load model\n",
    "# dictionary = corpora.Dictionary.load('model/dictionary.pkl')\n",
    "# tfidf = TfidfModel.load('model/tfidf.pkl')\n",
    "# lsi = LsiModel.load('model/lsi.pkl')\n",
    "# encoder = joblib.load('model/encoder.pkl')\n",
    "# grid_search = joblib.load('model/grid_search.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Judgmental (But Never Temperamental) Twitter Bot\n",
    "This Twitter bot will need to reply to users so we’ll need to set up a user auth. This time we'll need a Twitter user access token and access token secret. We’ll be sourcing our GIFs from [GIPHY](https://giphy.com/) for this proof of concept, in execution we would use custom assets and copy, so add in your [GIPHY API](https://developers.giphy.com/) key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIPHY_KEY = '<GIPHY_KEY>'\n",
    "twitterAuth_USER = tweepy.OAuthHandler(\n",
    "    '<CONSUMER_KEY>',\n",
    "    '<CONSUMER_SECRET>'\n",
    ")\n",
    "twitterAuth_USER.set_access_token(\n",
    "    '<ACCESS_TOKEN>',\n",
    "    '<ACCESS_TOKEN_SECRET>'\n",
    ")\n",
    "twitterAPI_USER = tweepy.API(twitterAuth_USER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s create a function that will take a user handle as input and output their archetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import TweepError\n",
    "\n",
    "def checkUser(handle):\n",
    "    # try to collect the timeline\n",
    "    try:\n",
    "        timeline = getTimeline(handle)\n",
    "    except TweepError as e:\n",
    "        return False\n",
    "    \n",
    "    # tokenize the timeline\n",
    "    tokens = []\n",
    "    for tweet in timeline:\n",
    "        tokens.extend(tokenize(tweet))\n",
    "        \n",
    "    # convert to a bag of words\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    \n",
    "    # vectorize the timeline\n",
    "    vector = lsi[tfidf[bow]]\n",
    "    array = tuples_array(vector, lsi.num_topics)\n",
    "    \n",
    "    # return the matching result\n",
    "    return encoder.classes_[grid_search.predict([array])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s set up the function that will reply to a given tweet, we’ll use [requests](http://docs.python-requests.org/en/master/) to make our GIPHY API calls and Tweepy to send the reply. We’ll be using the [random endpoint](https://developers.giphy.com/docs/#operation--gifs-random-get) to pull a GIF tagged with our archetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def sendGIF(tweet):\n",
    "    print(\"Replying to the tweet\")\n",
    "    handle = tweet.user.screen_name\n",
    "    archetype = checkUser(handle)\n",
    "\n",
    "    try:\n",
    "        # request a random GIF relevant to the archetype\n",
    "        data = requests.get(\"http://api.giphy.com/v1/gifs/random?api_key={}&tag={}\".format(GIPHY_KEY, archetype))\n",
    "        if data.status_code == 200:\n",
    "            # send a Twitter reply with a link to the GIF\n",
    "            gif = data.json()['data']['image_url']\n",
    "            reply = twitterAPI_USER.update_status(\n",
    "                \"@{} {}\".format(handle, gif),\n",
    "                in_reply_to_status_id = tweet.id_str\n",
    "            )\n",
    "            print(\"http://www.twitter.com/{}/status/{}\".format(reply.user.screen_name, reply.id))\n",
    "\n",
    "        else:\n",
    "            print(\"GIPHY has failed us\")\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "\n",
    "        if (str(e).lower().find('is a duplicate')):\n",
    "            print(\"Reply already sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter allows us to listen to a [stream of content filtered around specific keywords](https://developer.twitter.com/en/docs/tweets/filter-realtime/overview), be those hashtags, at-mentions, or otherwise. Provided that your bot is not receiving super-bowl level engagement, this endpoint should catch everything. We can access this endpoint through Tweepy which uses a StreamListener class to handle incoming tweets, we’ll create a subclass of this listener to add our own tweet handling functions. We’ll put each incoming tweet into a [queue](https://docs.python.org/2/library/queue.html) so that our listener doesn’t get backed up and handle each tweet in a separate thread in the order they were received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "if six.PY3:\n",
    "    from queue import Queue\n",
    "else:\n",
    "    from Queue import Queue\n",
    "import threading\n",
    "\n",
    "class BotListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, handler):\n",
    "        # initialize the queue and handler functions\n",
    "        self.q = Queue()\n",
    "        self.handler = handler\n",
    "        # start the thread for the queue\n",
    "        thread = threading.Thread(target=self.handleQueue)\n",
    "        thread.setDaemon(True)\n",
    "        thread.start()\n",
    "        # initialize the parent class\n",
    "        tweepy.StreamListener.__init__(self)\n",
    "    \n",
    "    def handleQueue(self):\n",
    "        # an endless loop that runs in a separate thread\n",
    "        while True:\n",
    "            # sends each tweet object to the handler function\n",
    "            tweet = self.q.get()\n",
    "            self.handler(tweet)\n",
    "            self.q.task_done()\n",
    "\n",
    "    def on_status(self, tweet):\n",
    "        # add tweet to the queue\n",
    "        self.q.put(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s start the bot listening around the hashtag _#MyArchetypeBot_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = BotListener(sendGIF)\n",
    "stream = tweepy.Stream(auth = twitterAuth_USER, listener = listener)\n",
    "stream.filter(track = [\"#MyArchetypeBot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it! Now when the bot notices a tweet with the given hashtag it will collect the user’s timeline and respond with a link to a GIF relevant to the user’s interests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
